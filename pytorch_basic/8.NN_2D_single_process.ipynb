{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        \n",
    "        #nn.Conv2d 接受一个4维的张量，即nSamples x nChannels x Height x Width\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:] #除去批处理维度的其他所有维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features*=s\n",
    "        return num_features\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 84])\n"
     ]
    }
   ],
   "source": [
    "#只需要定义 forward 函数，backward函数会在使用autograd时自动定义\n",
    "#可学习参数可以通过net.parameters()返回\n",
    "params = list(net.parameters())\n",
    "print(params[8].size())  # conv1's .weight\n",
    "#3x3,1channel,6nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.4684e-02,  3.4561e-02,  7.4980e-02,  8.5394e-02,  1.7106e-03,\n",
      "         4.7966e-02,  4.7496e-02, -2.7433e-03,  1.7400e-02,  5.5115e-02,\n",
      "         6.7925e-02,  3.5371e-02,  6.6226e-02,  1.1103e-02, -2.4573e-02,\n",
      "         5.6788e-04, -8.8122e-02,  7.8587e-02, -5.6167e-02, -4.6688e-02,\n",
      "         2.3285e-02, -4.4688e-02, -5.5120e-03, -9.0511e-02, -1.0067e-01,\n",
      "         3.4955e-02,  2.0192e-02, -5.1164e-02, -1.5932e-02, -8.5873e-02,\n",
      "         6.2057e-02, -2.0761e-03, -4.0981e-02,  7.2101e-02, -7.0125e-02,\n",
      "         5.1413e-02,  1.7200e-02, -7.5259e-02,  8.8434e-02, -4.6851e-02,\n",
      "         7.4234e-02, -4.1856e-02,  9.4501e-03,  3.6735e-02,  8.0596e-02,\n",
      "         9.0384e-02,  1.1378e-02,  3.8251e-03, -2.3081e-02,  1.0827e-01,\n",
      "         8.7125e-02, -4.9087e-02, -8.7020e-03, -7.2149e-02, -8.7786e-02,\n",
      "        -2.5918e-02,  1.3950e-02,  9.7916e-02, -9.8683e-02, -7.8135e-02,\n",
      "         2.7821e-02,  6.7591e-02,  5.0045e-02, -4.2832e-02,  6.3457e-05,\n",
      "         7.4131e-02, -7.9049e-02, -1.0833e-01, -5.7874e-02, -1.0430e-01,\n",
      "        -3.2709e-02,  7.1580e-02, -7.7645e-02, -8.4636e-02,  4.5545e-02,\n",
      "        -4.3048e-02, -9.6745e-02, -8.0649e-02, -1.5882e-02,  5.8090e-02,\n",
      "        -7.5642e-02, -1.5814e-02, -1.0545e-01, -9.5932e-03],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(params[8][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "print(params[1].size())  # conv2(maxpool)'s .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 6, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(params[2].size())  # conv3's .weight\n",
    "##3x3,6node input , 16node output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120])\n"
     ]
    }
   ],
   "source": [
    "print(params[5].size())  # conv6(ReLU)'s .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0550, -0.0035, -0.0233, -0.0223, -0.0566, -0.1032, -0.1182, -0.0413,\n",
      "          0.0774,  0.0759]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#nn.Conv2d 接受一个4维的张量，即nSamples x nChannels x Height x Width\n",
    "#如果是一个单独的样本，只需要使用input.unsqueeze(0)来添加一个“假的”批大小维度。\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#清零所有参数的梯度缓存，然后进行随机梯度的反向传播：\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1955, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=net(input)\n",
    "target=torch.randn(10)# 假的target数据\n",
    "target=target.view(1,-1)# 使目标值与数据值尺寸一致\n",
    "criterion=nn.MSELoss()#loss function\n",
    "\n",
    "loss=criterion(output,target)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "#      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "#      -> MSELoss\n",
    "#      -> loss\n",
    "loss.backward()\n",
    "#requires_grad=True以经计算好链式法则所需的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7ff1a218af50>\n",
      "<AddmmBackward object at 0x7ff21071fd10>\n",
      "<AccumulateGrad object at 0x7ff1a1ad5110>\n"
     ]
    }
   ],
   "source": [
    "#let us follow a few steps backward:\n",
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#需要清零现有的梯度，否则梯度将会与已有的梯度累加\n",
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad after backward\n",
      "tensor([-0.0169, -0.0205,  0.0029, -0.0282,  0.0163, -0.0046])\n"
     ]
    }
   ],
   "source": [
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#现在已经算了各层各节点的梯度，只欠更新权重\n",
    "#weight = weight - learning_rate * gradient\n",
    "#简单手动过程\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "    #f.data=f.data-f.grad.data*learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-6.4384e-02,  3.4786e-02,  7.5336e-02,  8.5394e-02,  1.9380e-03,\n",
      "         4.8232e-02,  4.7496e-02, -2.7433e-03,  1.7400e-02,  5.5858e-02,\n",
      "         6.8484e-02,  3.5920e-02,  6.6226e-02,  1.1103e-02, -2.4573e-02,\n",
      "         7.8164e-04, -8.8000e-02,  7.9080e-02, -5.6167e-02, -4.6352e-02,\n",
      "         2.3294e-02, -4.4688e-02, -5.5120e-03, -9.0511e-02, -1.0067e-01,\n",
      "         3.4955e-02,  2.0192e-02, -5.1127e-02, -1.5932e-02, -8.5873e-02,\n",
      "         6.2057e-02, -2.0351e-03, -4.0710e-02,  7.2788e-02, -7.0125e-02,\n",
      "         5.1413e-02,  1.7200e-02, -7.5106e-02,  8.9275e-02, -4.6848e-02,\n",
      "         7.4234e-02, -4.1856e-02,  9.4501e-03,  3.7328e-02,  8.1254e-02,\n",
      "         9.0384e-02,  1.1378e-02,  3.8732e-03, -2.2990e-02,  1.0827e-01,\n",
      "         8.7125e-02, -4.9087e-02, -8.7020e-03, -7.2149e-02, -8.7786e-02,\n",
      "        -2.5918e-02,  1.4983e-02,  9.8040e-02, -9.8683e-02, -7.8135e-02,\n",
      "         2.7821e-02,  6.7591e-02,  5.0045e-02, -4.2832e-02,  6.3457e-05,\n",
      "         7.4417e-02, -7.8386e-02, -1.0833e-01, -5.7546e-02, -1.0430e-01,\n",
      "        -3.2360e-02,  7.1580e-02, -7.7543e-02, -8.4578e-02,  4.6464e-02,\n",
      "        -4.3048e-02, -9.6335e-02, -7.9638e-02, -1.4951e-02,  5.8990e-02,\n",
      "        -7.5199e-02, -1.5404e-02, -1.0532e-01, -9.5932e-03],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(params[8][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在使用神经网络时，可能希望使用各种不同的更新规则，\n",
    "#如SGD、Nesterov-SGD、Adam、RMSProp等\n",
    "import torch.optim as optim\n",
    "# 创建优化器(optimizer）\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在训练的迭代中：\n",
    "optimizer.zero_grad()   # 清零梯度缓存\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # 更新参数\n",
    "#梯度是累加的,必须使用optimizer.zero_grad()手动清零的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
